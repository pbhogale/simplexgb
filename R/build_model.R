#' This function guesses sensible parameters for setting up the
#' xgb model. If the user has supplied parameters, the function
#' chooses the ones least likely to lead to overfitting.
#'
#' Objective functions and eval metrics are always taken
#' to be the user supplied values, if the suer has supplied them,
#' else they are guessed based on the type of the target variable.
#'
#' @param train_structure is the data structure produced by prepare_training_set
#' @param n_estimators minimum number of estimators
#' @param learning_rate maximum learning rate
#' @param depth minimum depth of trees
#' @param nrounds the number of rounds of training
#' @param objective_function
#' @param eval_metric
#' @export
guess_hyperparameters <- function(train_structure,
                                  n_estimators = 1000,
                                  learning_rate = 0.1,
                                  depth = 2,
                                  nrounds = 20,
                                  objective_function = NA,
                                  eval_metric = NA){
  hyperparameters <- list()
  hyperparameters[["depth"]] <- max(depth, floor(sqrt(ncol(train_structure$data))))
  hyperparameters[["n_estimators"]] <- max(n_estimators, exp(floor(log(nrow(train_structure$data)))-2))
  hyperparameters[["learning_rate"]] <- min(learning_rate, 1/(log(hyperparameters[["n_estimators"]]*hyperparameters[["depth"]])))
  class_target <- ("target_reference" %in% names(train_structure))
  if(class_target){
    hyperparameters[["objective_function"]] <- "multi:softprob"
    hyperparameters[["eval_metric"]] <- "mlogloss"
    hyperparameters[["num_class"]] <- train_structure$data[[train_structure$target_variable]] %>% dplyr::n_distinct()
  } else {
    hyperparameters[["objective_function"]] <- "reg:linear"
    hyperparameters[["eval_metric"]] <- "mae"
  }
  if(!is.na(objective_function)){
    hyperparameters[["objective_function"]] <- objective_function
  }
  if(!is.na(eval_metric)){
    hyperparameters[["eval_metric"]] <- eval_metric
  }
  hyperparameters[["nrounds"]] <- max(nrounds, floor(hyperparameters[["depth"]]*sqrt(hyperparameters[["n_estimators"]])))
  return(hyperparameters)
}


#' this function takes the train structure and hyperparameters, and an integer n
#' and does an n fold cross validation and returns the result.
#' @param train_structure the data structure generated by prepare_training_set
#' @param hyperparameters the list generated by guess_hyperparameters function
#' @param nfold number of folds of CV
#' @export
cross_validate <- function(train_structure, hyperparameters, nfold = 5){
  xgb_params <- list("objective" = hyperparameters[["objective_function"]],
                     "eval_metric" = hyperparameters[["eval_metric"]],
                     "eta" = hyperparameters[["learning_rate"]],
                     "max_depth" = hyperparameters[["depth"]],
                     "n_estimators" = hyperparameters[["n_estimators"]])
  if("num_class" %in% names(hyperparameters)){
    xgb_params[["num_class"]] <- hyperparameters[["num_class"]]
  }
  features <-Matrix::sparse.model.matrix(as.formula(paste(train_structure$target_variable, "~ .")),
                                  data = train_structure$data)[,-1]
  lab <- train_structure$data[[train_structure$target_variable]]
  dtrain <- xgboost::xgb.DMatrix(data = features, label = lab)
  cv_model <- xgboost::xgb.cv(params = xgb_params,
                              data = dtrain,
                              verbose = F,
                              nfold = nfold,
                              nrounds = hyperparameters[["nrounds"]],
                              prediction = T)
  ret_struct <- list()
  ret_struct[["cv_model"]] <- cv_model
  if("num_class" %in% names(hyperparameters)){
    OOF_prediction <- tibble::tibble(cv_model$pred) %>%
      dplyr::mutate(max_prob = max.col(., ties.method = "last")) %>%
      dplyr::mutate(label = lab+1)
    cm <- caret::confusionMatrix(factor(OOF_prediction$max_prob),
                          factor(OOF_prediction$label),
                          mode = "everything")
    ret_struct[["confusion_matrix"]] <- cm
    ret_struct[["metric"]] <- cv_model$evaluation_log$test_mlogloss_mean[length(cv_model$evaluation_log$test_mlogloss_mean)]
  } else {
    ret_struct[["metric"]] <- cv_model$evaluation_log$test_mae_mean[length(cv_model$evaluation_log$test_mae_mean)]
  }
  return(ret_struct)
}


#' this function returns a structure that contains the xgb model as well as
#' everything else it needs (normalization factors, levels of variables etc)
#' a predict function would need
#' @param train_structure the data structure generated by prepare_training_set
#' @param hyperparameters the list generated by guess_hyperparameters function
#' @export
train_model <- function(train_structure, hyperparameters){
  xgb_params <- list("objective" = hyperparameters[["objective_function"]],
                     "eval_metric" = hyperparameters[["eval_metric"]],
                     "eta" = hyperparameters[["learning_rate"]],
                     "max_depth" = hyperparameters[["depth"]],
                     "n_estimators" = hyperparameters[["n_estimators"]])
  if("num_class" %in% names(hyperparameters)){
    xgb_params[["num_class"]] <- hyperparameters[["num_class"]]
  }
  features <-Matrix::sparse.model.matrix(as.formula(paste(train_structure$target_variable, "~ .")),
                                         data = train_structure$data)[,-1]
  lab <- train_structure$data[[train_structure$target_variable]]
  dtrain <- xgboost::xgb.DMatrix(data = features, label = lab)
  xgbmodel <- xgboost::xgb.train(params = xgb_params,
                                 data = dtrain,
                                 verbose = F,
                                 nrounds = hyperparameters[["nrounds"]],
                                 prediction = T)
  model_structure <- list()
  model_structure[['model']] <- xgbmodel
  if('num_class' %in% names(hyperparameters)){
    model_structure[['target_reference']] <- train_structure[['target_reference']]
  }
  model_structure[['normalize_by']] <- train_structure[['normalize_by']]
  model_structure[['levels']] <- train_structure[['levels']]
  model_structure[['target_variable']] <- train_structure[['target_variable']]
  return(model_structure)
}

#' this function takes the model structure generated by train_model, along with
#' a test set in the same format as the untransformed *input* df
#' to the prepare_training_set function, to return a prediction vector
#' in the untransformed df.
#' @param model_structure
#' @param test_df
#' @export
get_predictions <- function (model_structure, test_df)
{
  levels_df <- model_structure[["levels"]]
  test_cols <- colnames(test_df)
  level_cols <- colnames(levels_df)
  for (i in 1:length(level_cols)) {
    if (!(level_cols[i] %in% test_cols))
      levels[[level_cols[[i]]]] <- NULL
  }
  test_df[[model_structure[["target_variable"]]]] <- NULL
  test_df <- rationalize_categoricals(test_df)
  norm_test_df <- normalize_df(test_df, facs_df = model_structure[["normalize_by"]],
                               target_variable = model_structure[["target_variable"]])
  norm_test_df <- rbind(levels_df,norm_test_df)
  norm_test_df[[model_structure[["target_variable"]]]] <- 0
  features <- Matrix::sparse.model.matrix(as.formula(paste(model_structure[["target_variable"]],"~ .")),
                                          data = norm_test_df, row.names = F)[, -1]
  dtest <- xgboost::xgb.DMatrix(data = features)
  preds <- predict(model_structure[["model"]], dtest)
  if (model_structure[["model"]][["params"]][["objective"]] ==
      "multi:softprob") {
    prob_matrix <- matrix(preds, nrow = nrow(norm_test_df),
                          byrow = T)
    predictions <- tibble::as_tibble(prob_matrix) %>% tail(nrow(test_df))
    if(length(as.character(model_structure[["target_reference"]][[1]]))==2){
      predictions <- predictions %>%
        mutate(V2 = 1-V1)
    }
    colnames(predictions) <- as.character(model_structure[["target_reference"]][[1]])
    cat_df <- predictions %>% tibble::rownames_to_column("row_id") %>%
      dplyr::mutate(row_id = as.numeric(row_id)) %>% tidyr::gather(category,
                                                                   value, -row_id) %>% dplyr::group_by(row_id) %>% dplyr::slice(which.max(value)) %>%
      dplyr::arrange(row_id)
    predictions[["category"]] <- cat_df[["category"]]
  }
  else {
    predictions <- tibble::tibble(prediction = preds[1:nrow(test_df)])
    colnames(predictions) <- model_structure[["target_variable"]]
  }
  return(predictions)
}
